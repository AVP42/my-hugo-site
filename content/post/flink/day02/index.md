---
title: "æ·±å…¥æµ…å‡ºFlink-ç¬¬äºŒå¤©"
date: "2021-03-11"
categories:
 - "æŠ€æœ¯"
tags:
 - "è½¯ä»¶å¼€å‘"
 - "python"
 - "å¼€æº"
toc: true
draft: false
math: true
diagram: true
---

## 1ï¸âƒ£ ä¸€ ã€è¯¾å‰å‡†å¤‡

1. æŒæ¡å‰é¢çš„flinkçŸ¥è¯†




$$
\begin{align*}
y = y(x,t) &= A e^{i\theta} \\
&= A (\cos \theta + i \sin \theta) \\
&= A (\cos(kx - \omega t) + i \sin(kx - \omega t)) \\
&= A\cos(kx - \omega t) + i A\sin(kx - \omega t)  \\
&= A\cos \Big(\frac{2\pi}{\lambda}x - \frac{2\pi v}{\lambda} t \Big) + i A\sin \Big(\frac{2\pi}{\lambda}x - \frac{2\pi v}{\lambda} t \Big)  \\
&= A\cos \frac{2\pi}{\lambda} (x - v t) + i A\sin \frac{2\pi}{\lambda} (x - v t)
\end{align*}
$$



$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$

{{% callout note %}}
A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
{{% /callout %}}



<div style="callout:success">
 Lorem ipsum
</div>





{{< spoiler text="Click to view the spoiler" >}} You found me! {{< /spoiler >}}




## 2ï¸âƒ£ äºŒ ã€è¯¾å ‚ä¸»é¢˜

1. æŒæ¡DataStreamå¸¸è§çš„APIå¼€å‘
2. æŒæ¡DataSetå¸¸è§çš„APIå¼€å‘



## 3ï¸âƒ£ ä¸‰ ã€è¯¾ç¨‹ç›®æ ‡

1. æŒæ¡å¸¸è§çš„DataStreamå¸¸è§çš„source
2. æŒæ¡å¸¸è§çš„DataStreamçš„transformationæ“ä½œ
3. æŒæ¡å¸¸è§çš„DataStreamçš„sinkæ“ä½œ
4. äº†è§£å…¥é—¨çš„DataSet APIç®—å­






## 4ï¸âƒ£ å›› ã€çŸ¥è¯†è¦ç‚¹

### ğŸ“–  1. DataStream çš„ç¼–ç¨‹æ¨¡å‹ 

* DataStream çš„ç¼–ç¨‹æ¨¡å‹åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼šEnvironmentã€DataSourceã€Transformationã€Sink

  ![1587215224155](./images/1587215224155.png)



### ğŸ“–  2. Flinkçš„DataSourceæ•°æ®æº

#### 2.1 åŸºäºæ–‡ä»¶


> readTextFile(path)
> è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼Œæ–‡ä»¶éµå¾ªTextInputFormatè¯»å–è§„åˆ™ï¼Œé€è¡Œè¯»å–å¹¶è¿”å›ã€‚


* æ¡ˆä¾‹

```scala
package com.kaikeba.demo1
import org.apache.flink.api.scala.{ DataSet, ExecutionEnvironment}

//todo:  scalaå¼€å‘flinkçš„æ‰¹å¤„ç†ç¨‹åº


object FlinkFileCount {
   
  def main(args: Array[String]): Unit = {

    //todo:1ã€æ„å»ºFlinkçš„æ‰¹å¤„ç†ç¯å¢ƒ
    val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

    //todo:2ã€è¯»å–æ•°æ®æ–‡ä»¶
    val fileDataSet: DataSet[String] = env.readTextFile("d:\\words.txt")

    import org.apache.flink.api.scala._

    //todo: 3ã€å¯¹æ•°æ®è¿›è¡Œå¤„ç†
    val resultDataSet: AggregateDataSet[(String, Int)] = fileDataSet
                                                                    .flatMap(x=> x.split(" "))
                                                                    .map(x=>(x,1))
                                                                    .groupBy(0)
                                                                    .sum(1)

    //todo: 4ã€æ‰“å°ç»“æœ
    resultDataSet.print()

    //todo: 5ã€ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    resultDataSet.writeAsText("d:\\result")

    env.execute("FlinkFileCount")
  }
}

```


#### 2.2 åŸºäºsocket

~~~
socketTextStream
ä»sockerä¸­è¯»å–æ•°æ®ï¼Œå…ƒç´ å¯ä»¥é€šè¿‡ä¸€ä¸ªåˆ†éš”ç¬¦åˆ‡å¼€ã€‚
~~~

* æ¡ˆä¾‹

```mermaid
  %% Example of sequence diagram
  sequenceDiagram
    Alice->>Bob: Hello Bob, how are you?
    alt is sick
    Bob->>Alice: Not so good :(
    else is well
    Bob->>Alice: Feeling fresh like a daisy
    end
    opt Extra response
    Bob->>Alice: Thanks for asking
    end
```

  

  

  ~~~scala
  package com.kaikeba.demo1
  
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  
  //todo: é€šè¿‡scalaå¼€å‘flinkæµå¤„ç†ä½œä¸š
  object WordCountStreamScala {
  
    def main(args: Array[String]): Unit = {
  
      //æ„å»ºæµå¤„ç†çš„ç¯å¢ƒ
      val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
  
      //ä»socketè·å–æ•°æ®
      val sourceStream: DataStream[String] = env.socketTextStream("node01",9999)
  
      //å¯¼å…¥éšå¼è½¬æ¢çš„åŒ…
      import org.apache.flink.api.scala._
  
      //å¯¹æ•°æ®è¿›è¡Œå¤„ç†
      val result: DataStream[(String, Int)] = sourceStream
        .flatMap(x => x.split(" ")) //æŒ‰ç…§ç©ºæ ¼åˆ‡åˆ†
        .map(x => (x, 1))           //æ¯ä¸ªå•è¯è®¡ä¸º1
        .keyBy(0)                   //æŒ‰ç…§ä¸‹æ ‡ä¸º0çš„å•è¯è¿›è¡Œåˆ†ç»„
        .sum(1)                     //æŒ‰ç…§ä¸‹æ ‡ä¸º1ç´¯åŠ ç›¸åŒå•è¯å‡ºç°çš„æ¬¡æ•°
  
      //å¯¹æ•°æ®è¿›è¡Œæ‰“å°
      result.print()
  
      //å¼€å¯ä»»åŠ¡
      env.execute("WordCountStreamScala")
  
  
    }
  }
  ~~~


#### 2.3 åŸºäºé›†åˆ

~~~
fromCollection(Collection)
é€šè¿‡collectioné›†åˆåˆ›å»ºä¸€ä¸ªæ•°æ®æµï¼Œé›†åˆä¸­çš„æ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯ç›¸åŒç±»å‹çš„ã€‚

~~~

* æ¡ˆä¾‹

  ~~~scala
  package com.kaikeba.demo2
  
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  import org.apache.flink.api.scala._
  
  //todo: åŸºäºæ•°ç»„æˆ–è€…é›†åˆæ„å»ºDataStream
  object StreamingSourceFromCollection {
  
    def main(args: Array[String]): Unit = {
      //todo: 1ã€è·å–æµå¼å¤„ç†ç¯å¢ƒ
      val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
  
      //todo: 2ã€å‡†å¤‡æ•°æ®æº--æ•°ç»„
      val array = Array("hello world","world spark","flink test","spark hive","test")
      val fromArray: DataStream[String] = environment.fromCollection(array)
  
  
      //  val value: DataStream[String] = environment.fromElements("hello world")
      //todo: 3ã€æ•°æ®å¤„ç†
      val resultDataStream: DataStream[(String, Int)] = fromArray
                                                                .flatMap(x => x.split(" "))
                                                                .map(x =>(x,1))
                                                                .keyBy(0)
                                                                .sum(1)
  
      //todo: 4ã€æ‰“å°
      resultDataStream.print()
  
      //todo: 5ã€å¯åŠ¨
      environment.execute()
  
  
    }
  }
  ~~~


#### 2.4 è‡ªå®šä¹‰è¾“å…¥

~~~
addSource 
å¯ä»¥å®ç°è¯»å–ç¬¬ä¸‰æ–¹æ•°æ®æºçš„æ•°æ®
~~~

æŒæ¡å‰é¢çš„flinkçŸ¥è¯†
è‡ªå®šä¹‰å•å¹¶è¡Œåº¦æ•°æ®æº

* è‡ªå®šä¹‰å•å¹¶è¡Œåº¦æ•°æ®æº

  * ç»§æ‰¿SourceFunctionæ¥è‡ªå®šä¹‰å•å¹¶è¡Œåº¦source

  * ä»£ç å¼€å‘

    ~~~scala
    package com.kaikeba.demo2
    
    import org.apache.flink.streaming.api.functions.source.SourceFunction
    import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
    import org.apache.log4j.{Level, Logger}
    
    /**
      * todo: è‡ªå®šä¹‰å•å¹¶è¡Œåº¦source
      *
      */
    object MySourceRun {
      Logger.getLogger("org").setLevel(Level.ERROR)
    
      def main(args: Array[String]): Unit = {
        //todo: æ„å»ºæµå¤„ç†ç¯å¢ƒ
        val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
        import org.apache.flink.api.scala._
    
        //todo: æ·»åŠ è‡ªå®šä¹‰source    è·å–æ•°æ®æºçš„source task ä¸ªæ•°ä¸º1ï¼Œå•å¹¶è¡Œåº¦
        val getSource: DataStream[Long] = environment.addSource(new MySource)
    
    
        //todo: è¿‡æ»¤å¤„ç†
        val resultStream: DataStream[Long] = getSource.filter(x => x %2 ==0)
        resultStream.setParallelism(1).print()
    
    
        //todoï¼š å¯åŠ¨
        environment.execute()
      }
    }
    
    //todo: è‡ªå®šä¹‰å•å¹¶è¡Œåº¦source
    class MySource extends SourceFunction[Long] {
      private var number = 1L
      private var isRunning = true
    
      override def run(sourceContext: SourceFunction.SourceContext[Long]): Unit = {
        //todo: ä¸æ–­çš„äº§ç”Ÿæ•°æ®
        while (isRunning){
            number += 1
            sourceContext.collect(number)
            Thread.sleep(1000)
        }
      }
      override def cancel(): Unit = {
        isRunning = false
      }
    }
    
    ~~~

* è‡ªå®šä¹‰å¤šå¹¶è¡Œåº¦æ•°æ®æº

  * ç»§æ‰¿ParallelSourceFunctionæ¥è‡ªå®šä¹‰å¤šå¹¶è¡Œåº¦çš„source

  * ä»£ç å¼€å‘

    ~~~scala
    package com.kaikeba.demo2
    
    import org.apache.flink.streaming.api.functions.source.{ParallelSourceFunction, RichSourceFunction, SourceFunction}
    import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
    import org.apache.log4j.{Level, Logger}
    
    /**
      * todoï¼š è‡ªå®šä¹‰å¤šå¹¶è¡Œåº¦çš„source
      */
    object MyMultipartSourceRun {
       Logger.getLogger("org").setLevel(Level.ERROR)
    
      def main(args: Array[String]): Unit = {
         //todo: 1ã€æ„å»ºæµå¤„ç†ç¯å¢ƒ
        val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
        import org.apache.flink.api.scala._
    
        //todo: 2ã€æ·»åŠ source
        val getSource: DataStream[Long] = environment.addSource(new MultipartSource).setParallelism(2)
    
        //todo: 3ã€è¿‡æ»¤å¤„ç†
        val resultStream: DataStream[Long] = getSource.filter(x => x %2 ==0)
    
        //todo: 4ã€æ‰“å°è¾“å‡º
        resultStream.setParallelism(1).print()
    
        //todo: 5ã€å¯åŠ¨
        environment.execute()
      }
    }
    
    
    //todo: è‡ªå®šä¹‰å¤šå¹¶è¡Œåº¦çš„source
    class MultipartSource  extends ParallelSourceFunction[Long]{
      private var number = 1L
      private var isRunning = true
    
      override def run(sourceContext: SourceFunction.SourceContext[Long]): Unit = {
        while(true){
          number +=1
          sourceContext.collect(number)
          Thread.sleep(1000)
        }
    
      }
    
      override def cancel(): Unit = {
        isRunning = false
    
      }
    }
    
    ~~~


* æ­¤å¤–ç³»ç»Ÿå†…ç½®æä¾›äº†ä¸€æ‰¹connectorsï¼Œè¿æ¥å™¨ä¼šæä¾›å¯¹åº”çš„sourceæ”¯æŒ
  * [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/kafka.html) (source/sink)  **åé¢é‡ç‚¹åˆ†æ**
  * [Apache Cassandra](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/cassandra.html) (sink)
  * [Amazon Kinesis Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/kinesis.html) (source/sink)
  * [Elasticsearch](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/elasticsearch.html) (sink)
  * [Hadoop FileSystem](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/filesystem_sink.html) (sink)
  * [RabbitMQ](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/rabbitmq.html) (source/sink)
  * [Apache NiFi](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/nifi.html) (source/sink)
  * [Twitter Streaming API](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/twitter.html) (source)





### ğŸ“–  3. Flinkçš„Sinkæ•°æ®ç›®æ ‡

- writeAsText()ï¼šå°†å…ƒç´ ä»¥å­—ç¬¦ä¸²å½¢å¼é€è¡Œå†™å…¥ï¼Œè¿™äº›å­—ç¬¦ä¸²é€šè¿‡è°ƒç”¨æ¯ä¸ªå…ƒç´ çš„toString()æ–¹æ³•æ¥è·å–
- print() / printToErr()ï¼šæ‰“å°æ¯ä¸ªå…ƒç´ çš„toString()æ–¹æ³•çš„å€¼åˆ°æ ‡å‡†è¾“å‡ºæˆ–è€…æ ‡å‡†é”™è¯¯è¾“å‡ºæµä¸­
- è‡ªå®šä¹‰è¾“å‡ºaddSinkã€kafkaã€redisã€‘
- æˆ‘ä»¬å¯ä»¥é€šè¿‡sinkç®—å­ï¼Œå°†æˆ‘ä»¬çš„æ•°æ®å‘é€åˆ°æŒ‡å®šçš„åœ°æ–¹å»ï¼Œä¾‹å¦‚kafkaæˆ–è€…redisæˆ–è€…hbaseç­‰ç­‰ï¼Œå‰é¢æˆ‘ä»¬å·²ç»ä½¿ç”¨è¿‡å°†æ•°æ®æ‰“å°å‡ºæ¥è°ƒç”¨print()æ–¹æ³•ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥å®ç°è‡ªå®šä¹‰sinkå°†æˆ‘ä»¬çš„æ•°æ®å‘é€åˆ°redisé‡Œé¢å»
  - [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html) (source/sink)
  - [Apache Cassandra](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/cassandra.html) (sink)
  - [Amazon Kinesis Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kinesis.html) (source/sink)
  - [Elasticsearch](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/elasticsearch.html) (sink)
  - [Hadoop FileSystem](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/filesystem_sink.html) (sink)
  - [RabbitMQ](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/rabbitmq.html) (source/sink)
  - [Apache NiFi](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/nifi.html) (source/sink)
  - [Twitter Streaming API](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/twitter.html) (source)
  - [Google PubSub](https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/pubsub.html) (source/sink)



#### 3.1  Flinkå†™æ•°æ®åˆ°redisä¸­

* å¯¼å…¥flinkæ•´åˆredisçš„jaråŒ…

  ~~~xml
  <dependency>
  Â Â Â  <groupId>org.apache.bahir</groupId>
  Â Â Â  <artifactId>flink-connector-redis_2.11</artifactId>
  Â Â Â  <version>1.0</version>
  </dependency>
  ~~~

* ä»£ç å¼€å‘

  ~~~scala
  package com.kaikeba.redis
  
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  import org.apache.flink.streaming.connectors.redis.RedisSink
  import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig
  import org.apache.flink.streaming.connectors.redis.common.mapper.{RedisCommand, RedisCommandDescription, RedisMapper}
  import org.apache.flink.api.scala._
  
  //todo: Flinkå®æ—¶ç¨‹åºå¤„ç†ä¿å­˜ç»“æœåˆ°redisä¸­
  object Stream2Redis {
  
    def main(args: Array[String]): Unit = {
      //todo: 1ã€è·å–ç¨‹åºå…¥å£ç±»
      val executionEnvironment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
  
  
      //todo: 2ã€ç»„ç»‡æ•°æ®
      val streamSource: DataStream[String] = executionEnvironment.fromElements("1 hadoop","2 spark","3 flink")
  
      //todo: 3ã€æ•°æ®å¤„ç†
      //å°†æ•°æ®åŒ…è£…æˆä¸ºkey,valueå¯¹å½¢å¼çš„tuple
      val tupleValue: DataStream[(String, String)] = streamSource.map(x =>(x.split(" ")(0),x.split(" ")(1)))
  
  
      //todo: 4ã€æ„å»ºRedisSink
      val builder = new FlinkJedisPoolConfig.Builder
  
        //è®¾ç½®rediså®¢æˆ·ç«¯å‚æ•°
        builder.setHost("node01")
        builder.setPort(6379)
        builder.setPassword("123456")
        builder.setTimeout(5000)
        builder.setMaxTotal(50)
        builder.setMaxIdle(10)
        builder.setMinIdle(5)
  
      val config: FlinkJedisPoolConfig = builder.build()
  
      //è·å–redis  sink
      val redisSink = new RedisSink[Tuple2[String,String]](config,new MyRedisMapper)
  
      //todo: 5ã€ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„sinkï¼Œå®ç°æ•°æ®å†™å…¥åˆ°redisä¸­
      tupleValue.addSink(redisSink)
  
      //todo: 6ã€æ‰§è¡Œç¨‹åº
      executionEnvironment.execute("redisSink")
    }
  }
  
  //todo: å®šä¹‰ä¸€ä¸ªRedisMapperç±»
  class MyRedisMapper  extends RedisMapper[Tuple2[String,String]]{
  
    override def getCommandDescription: RedisCommandDescription = {
      //è®¾ç½®æ’å…¥æ•°æ®åˆ°redisçš„å‘½ä»¤
      new RedisCommandDescription(RedisCommand.SET)
  
  
    }
    //todo: æŒ‡å®škey
    override def getKeyFromData(data: (String, String)): String = {
      data._1
  
    }
  
    //todo: æŒ‡å®švalue
    override def getValueFromData(data: (String, String)): String = {
      data._2
  
    }
  }
  
  ~~~



### ğŸ“–  4. DataStream è½¬æ¢ç®—å­

* é€šè¿‡ä»ä¸€ä¸ªæˆ–å¤šä¸ª DataStream ç”Ÿæˆæ–°çš„ DataStream çš„è¿‡ç¨‹è¢«ç§°ä¸º Transformation æ“ä½œã€‚åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ï¼Œæ¯ç§æ“ä½œç±»å‹è¢«å®šä¹‰ä¸ºä¸åŒçš„ Operator, Flink ç¨‹åºèƒ½å¤Ÿå°†å¤šä¸ª Transformation ç»„æˆä¸€ä¸ª DataFlow çš„æ‹“æ‰‘ã€‚ 

* DataStream å®˜ç½‘è½¬æ¢ç®—å­æ“ä½œï¼š

  <https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html>


#### 4.1 mapã€filter

~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}

//todo: æµ‹è¯• mapã€filter
object MapFilter {

  def main(args: Array[String]): Unit = {
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    import  org.apache.flink.api.scala._
    val sourceStream: DataStream[Int] = environment.fromElements(1,2,3,4,5,6)

    val mapStream: DataStream[Int] = sourceStream.map(x =>x*10)

    val resultStream: DataStream[Int] = mapStream.filter(x => x%2 ==0)
    resultStream.print()
    
    environment.execute()
  }
}

~~~

#### 4.2 flatMapã€keyByã€sum

~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.api.scala._
/**
  * ä½¿ç”¨æ»‘åŠ¨çª—å£
  * æ¯éš”1ç§’é’Ÿç»Ÿè®¡æœ€è¿‘2ç§’é’Ÿçš„æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°
  */
object FlinkStream {

  def main(args: Array[String]): Unit = {
    //todo: è·å–ç¨‹åºå…¥å£ç±»
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    environment.setParallelism(1)

    //todo: ä»socketå½“ä¸­è·å–æ•°æ®
    val resultDataStream: DataStream[String] = environment.socketTextStream("node01",9999)

    //todo: å¯¹æ•°æ®è¿›è¡Œè®¡ç®—æ“ä½œ
    val resultData: DataStream[(String, Int)] = resultDataStream
      .flatMap(x => x.split(" ")) //æŒ‰ç…§ç©ºæ ¼è¿›è¡Œåˆ‡åˆ†
      .map(x => (x, 1))  //ç¨‹åºå‡ºç°ä¸€æ¬¡è®°åš1
      .keyBy(0)  //æŒ‰ç…§ä¸‹æ ‡ä¸º0çš„å•è¯è¿›è¡Œç»Ÿè®¡
      .timeWindow(Time.seconds(2), Time.seconds(1)) //æ¯éš”ä¸€ç§’é’Ÿè®¡ç®—ä¸€æ¬¡å‰ä¸¤ç§’é’Ÿçš„å•è¯å‡ºç°çš„æ¬¡æ•°
      .sum(1)
      resultData.print()

    //todo: æ‰§è¡Œç¨‹åº
    environment.execute()

  }

}
~~~



#### 4.3 reduce

* æ˜¯å°†è¾“å…¥çš„ KeyedStream æµé€šè¿‡ä¼ å…¥çš„ç”¨æˆ·è‡ªå®šä¹‰çš„ReduceFunctionæ»šåŠ¨åœ°è¿›è¡Œæ•°æ®èšåˆå¤„ç†


~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.streaming.api.scala.{DataStream, KeyedStream, StreamExecutionEnvironment}

//todo: æµ‹è¯• reduce
object ReduceStream {
  def main(args: Array[String]): Unit = {

    //todoï¼šæ„å»ºStreamExecutionEnvironment
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    import org.apache.flink.api.scala._
    val sourceStream: DataStream[(String,Int)] = environment.fromElements(("a",1),("a",2),("b",2),("b",3),("c",2))

    //todo; æŒ‡å®šç¬¬ä¸€ä¸ªå­—æ®µä¸ºåˆ†åŒºKey
    val keyByStream: KeyedStream[(String, Int), Tuple] = sourceStream.keyBy(0)

    //val reduce: DataStream[(String, Int)] = keyByStream.reduce((x,y)=>(y._1,x._2+y._2))

    //todo: æ»šåŠ¨å¯¹ç¬¬äºŒä¸ªå­—æ®µè¿›è¡Œreduceç›¸åŠ æ±‚å’Œ
   val resultStream: DataStream[(String, Int)] = keyByStream.reduce((t1,t2)=>(t1._1,t1._2+t2._2))

    //todo: æ‰“å°
    resultStream.print()

    //todo: å¯åŠ¨
    environment.execute()
  }
}

~~~





#### 4.4 union

* æŠŠ2ä¸ªæµçš„æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œ2ä¸ªæµçš„æ•°æ®ç±»å‹å¿…é¡»ä¿æŒä¸€è‡´

~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.api.scala._

//todo: æµ‹è¯•unionç®—å­
object UnionStream {
  def main(args: Array[String]): Unit = {
    //todo: æ„å»ºStreamExecutionEnvironment
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    //todo: æ„å»º2ä¸ªæ•°æ®æµ
    val firstStream: DataStream[String] = environment.fromCollection(Array("hello spark","hello flink"))
    val secondStream: DataStream[String] = environment.fromCollection(Array("hadoop spark","hive flink"))

    //todo: ä¸¤ä¸ªæµåˆå¹¶æˆä¸ºä¸€ä¸ªæµï¼Œå¿…é¡»ä¿è¯ä¸¤ä¸ªæµå½“ä¸­çš„æ•°æ®ç±»å‹æ˜¯ä¸€è‡´çš„
    val resultStream: DataStream[String] = firstStream.union(secondStream)

    //todo: æ‰“å°
    resultStream.print()

    //todo: å¯åŠ¨
    environment.execute()
  }
}

~~~



#### 4.5 connect

*  å’Œunionç±»ä¼¼ï¼Œä½†æ˜¯åªèƒ½è¿æ¥ä¸¤ä¸ªæµï¼Œä¸¤ä¸ªæµçš„æ•°æ®ç±»å‹å¯ä»¥ä¸åŒ

~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.streaming.api.functions.co.CoFlatMapFunction
import org.apache.flink.streaming.api.scala.{ConnectedStreams, DataStream, StreamExecutionEnvironment}
import org.apache.flink.util.Collector

/**
  * todo: connect,conMapå’ŒconFlatMapä½¿ç”¨
  * å’Œunionç±»ä¼¼ï¼Œä½†æ˜¯åªèƒ½è¿æ¥ä¸¤ä¸ªæµï¼Œä¸¤ä¸ªæµçš„æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œ
  * ä¼šå¯¹ä¸¤ä¸ªæµä¸­çš„æ•°æ®åº”ç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
  */
object ConnectStream {
  def main(args: Array[String]): Unit = {
    //todo: æ„å»ºStreamExecutionEnvironment
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    environment.setParallelism(1)

    import org.apache.flink.api.scala._
    //todoï¼šç¬¬ä¸€ä¸ªæµæ•°æ®
    val firstStream: DataStream[String] = environment.fromCollection(Array("hello world","spark flink"))
    //todoï¼šç¬¬äºŒä¸ªæµæ•°æ®
    val secondStream: DataStream[Int] = environment.fromCollection(Array(1,2,3,4))

    //todo:è°ƒç”¨connectæ–¹æ³•è¿æ¥å¤šä¸ªDataStream
    val connectStream: ConnectedStreams[String, Int] = firstStream.connect(secondStream)

    //ConnectedStreamsæµè¿›è¡Œmapæ“ä½œ
   //val unionStream: DataStream[Any] = connectStream.map(x => x + "abc",y => y*2)

    //todo: ConnectedStreamsæµè¿›è¡ŒflatMapæ“ä½œ
    val coFlatMapStream: DataStream[String] = connectStream.flatMap(new CoFlatMapFunction[String, Int, String] {
         //todo: æ“ä½œç¬¬ä¸€ä¸ªæµä¸­çš„æ•°æ®
      override def flatMap1(value: String, out: Collector[String]): Unit = {
        out.collect(value.toUpperCase())
      }

        //todo: æ“ä½œç¬¬äºŒä¸ªæµä¸­çš„æ•°æ®
      override def flatMap2(value: Int, out: Collector[String]): Unit = {
        out.collect( value * 2 + "")
      }
    })

   // unionStream.print()
    coFlatMapStream.print()

    //todo: å¼€å¯ä»»åŠ¡
    environment.execute()
  }
}

~~~



#### 4.6 splitã€select

* æ ¹æ®è§„åˆ™æŠŠä¸€ä¸ªæ•°æ®æµåˆ‡åˆ†ä¸ºå¤šä¸ªæµ

~~~scala
package com.kaikeba.streamOperator

import java.{lang, util}

import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.collector.selector.OutputSelector
import org.apache.flink.streaming.api.scala.{DataStream, SplitStream, StreamExecutionEnvironment}
import org.apache.log4j.{Level, Logger}

/**
  *  todo: æ ¹æ®è§„åˆ™æŠŠä¸€ä¸ªæ•°æ®æµåˆ‡åˆ†ä¸ºå¤šä¸ªæµ
  *  åº”ç”¨åœºæ™¯ï¼š
  * å¯èƒ½åœ¨å®é™…å·¥ä½œä¸­ï¼Œæºæ•°æ®æµä¸­æ··åˆäº†å¤šç§ç±»ä¼¼çš„æ•°æ®ï¼Œå¤šç§ç±»å‹çš„æ•°æ®å¤„ç†è§„åˆ™ä¸ä¸€æ ·ï¼Œæ‰€ä»¥å°±å¯ä»¥åœ¨æ ¹æ®ä¸€å®šçš„è§„åˆ™ï¼Œ
  * æŠŠä¸€ä¸ªæ•°æ®æµåˆ‡åˆ†æˆå¤šä¸ªæ•°æ®æµï¼Œè¿™æ ·æ¯ä¸ªæ•°æ®æµå°±å¯ä»¥ä½¿ç”¨ä¸åŒçš„å¤„ç†é€»è¾‘äº†
  */
object SplitAndSelect {
  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {
    //todo: æ„å»ºStreamExecutionEnvironment
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    environment.setParallelism(1)


      //todo: æ„å»ºDataStream
    val firstStream: DataStream[String] = environment.fromCollection(Array("hadoop hive","spark flink"))

    //todo: å¯¹æ•°æ®æµè¿›è¡Œsplitåˆ‡åˆ†æ“ä½œ
    val selectStream: SplitStream[String] = firstStream.split(new OutputSelector[String] {

      override def select(value: String): lang.Iterable[String] = {
        var list = new util.ArrayList[String]()
        //todo: å¦‚æœåŒ…å«helloå­—ç¬¦ä¸²
        if (value.contains("hadoop")) {
            //å­˜æ”¾åˆ°ä¸€ä¸ªå«åšfirstçš„streamé‡Œé¢å»
            list.add("first")
        //todo: ä¸åŒ…å«helloå­—ç¬¦ä¸²
        }else{
          //å¦åˆ™å­˜æ”¾åˆ°ä¸€ä¸ªå«åšsecondçš„streamé‡Œé¢å»
           list.add("second")
        }
        list
      }
    })

    //todo: è·å–firstè¿™ä¸ªstream
    selectStream.select("first").print("contains hadoop")

    //todo: è·å–secondè¿™ä¸ªstream
    selectStream.select("second").print("not contains hadoop")

    //todo: æ‰§è¡Œä»»åŠ¡
    environment.execute("SplitAndSelect")

  }
}
~~~



#### 4.7 é‡åˆ†åŒºç®—å­

* ==é‡ç®—å­å…è®¸æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œé‡æ–°åˆ†åŒºï¼Œæˆ–è€…è§£å†³æ•°æ®å€¾æ–œç­‰é—®é¢˜==
  - Random Partitioning 

    - éšæœºåˆ†åŒº
      -  æ ¹æ®éšæœºçš„åˆ†é…å…ƒç´ ç»™ä¸‹æ¸¸taskï¼ˆç±»ä¼¼äºrandom.nextInt(5)ï¼Œ0 - 5 åœ¨æ¦‚ç‡ä¸Šéšæœºçš„ï¼‰
      - dataStream.shuffle()

  - Rebalancing 

    - å‡åŒ€åˆ†åŒº
      - åˆ†åŒºå…ƒç´ å¾ªç¯ï¼Œæ¯ä¸ªåˆ†åŒºåˆ›å»ºç›¸ç­‰çš„è´Ÿè½½ã€‚æ•°æ®å‘ç”Ÿå€¾æ–œçš„æ—¶å€™å¯ä»¥ç”¨äºæ€§èƒ½ä¼˜åŒ–ã€‚
      - å¯¹æ•°æ®é›†è¿›è¡Œå†å¹³è¡¡ï¼Œé‡åˆ†åŒºï¼Œæ¶ˆé™¤æ•°æ®å€¾æ–œ
      - dataStream.rebalance()

  - Rescalingï¼š

    - è·Ÿrebalanceæœ‰ç‚¹ç±»ä¼¼ï¼Œä½†ä¸æ˜¯å…¨å±€çš„ï¼Œè¿™ç§æ–¹å¼ä»…å‘ç”Ÿåœ¨ä¸€ä¸ªå•ä¸€çš„èŠ‚ç‚¹ï¼Œå› æ­¤æ²¡æœ‰è·¨ç½‘ç»œçš„æ•°æ®ä¼ è¾“ã€‚

      - dataStream.rescale()

  - Custom partitioningï¼šè‡ªå®šä¹‰åˆ†åŒº

    - è‡ªå®šä¹‰åˆ†åŒºéœ€è¦å®ç°Partitioneræ¥å£  
      - dataStream.partitionCustom(partitioner, "someKey")
      - æˆ–è€…dataStream.partitionCustom(partitioner, 0);

  - Broadcastingï¼šå¹¿æ’­å˜é‡ï¼Œåé¢è¯¦ç»†è®²è§£





##### 4.7.1 å¯¹filterä¹‹åçš„æ•°æ®è¿›è¡Œé‡æ–°åˆ†åŒº

~~~scala
package com.kaikeba.streamOperator

import org.apache.flink.api.common.functions.RichMapFunction
import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.log4j.{Level, Logger}

//todo: å¯¹filterä¹‹åçš„æ•°æ®è¿›è¡Œé‡æ–°åˆ†åŒº
object FlinkPartition {
  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI()

    val dataStream: DataStream[Int] = environment.fromCollection(1 to 100)

    val filterStream: DataStream[Int] = dataStream.filter(x => x>10)

        //.shuffle  //éšæœºçš„é‡æ–°åˆ†å‘æ•°æ®,ä¸Šæ¸¸çš„æ•°æ®ï¼Œéšæœºçš„å‘é€åˆ°ä¸‹æ¸¸çš„åˆ†åŒºé‡Œé¢å»
       .rebalance //å¯¹æ•°æ®é‡æ–°è¿›è¡Œåˆ†åŒºï¼Œæ¶‰åŠåˆ°shuffleçš„è¿‡ç¨‹
       //.rescale   //è·Ÿrebalanceæœ‰ç‚¹ç±»ä¼¼ï¼Œä½†ä¸æ˜¯å…¨å±€çš„ï¼Œè¿™ç§æ–¹å¼ä»…å‘ç”Ÿåœ¨ä¸€ä¸ªå•ä¸€çš„èŠ‚ç‚¹ï¼Œå› æ­¤æ²¡æœ‰è·¨ç½‘ç»œçš„æ•°æ®ä¼ è¾“ã€‚
     //å¸¦æœ‰Richçš„ç±»ï¼Œè¡¨ç¤ºå¯Œå‡½æ•°ç±»ï¼Œå®ƒçš„åŠŸèƒ½æ¯”è¾ƒå¼ºå¤§ï¼Œåœ¨å†…éƒ¨æ˜¯å¯ä»¥è·å–stateã€åˆ†å¸ƒå¼ç¼“å­˜ã€å¹¿æ’­å˜é‡ã€è¿è¡Œæ—¶çš„ä¸Šä¸‹æ–‡å¯¹è±¡ç­‰ç­‰
    val resultStream: DataStream[(Int, Int)] = filterStream.map(new RichMapFunction[Int, (Int, Int)] {
      override def map(value: Int): (Int, Int) = {
        Thread.sleep(100000)
         //è·å–ä»»åŠ¡idï¼Œä»¥åŠvalue
        (getRuntimeContext.getIndexOfThisSubtask, value)

      }
    })
    //todo: æ‰“å°
    resultStream.print()
    //todo: å¯åŠ¨
    environment.execute()

  }
}

~~~



##### 4.7.2  è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥

- å¦‚æœä»¥ä¸Šçš„å‡ ç§åˆ†åŒºæ–¹å¼è¿˜æ²¡æ³•æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥æ¥å®ç°æ•°æ®çš„åˆ†åŒº

- éœ€æ±‚

  - è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥ï¼Œå®ç°ä¸åŒåˆ†åŒºçš„æ•°æ®å‘é€åˆ°ä¸åŒåˆ†åŒºé‡Œé¢å»è¿›è¡Œå¤„ç†ï¼Œå°†åŒ…å«helloçš„å­—ç¬¦ä¸²å‘é€åˆ°ä¸€ä¸ªåˆ†åŒºé‡Œé¢å»ï¼Œå…¶ä»–çš„å‘é€åˆ°å¦å¤–ä¸€ä¸ªåˆ†åŒºé‡Œé¢å»

- å®šä¹‰åˆ†åŒºç±»

  ~~~scala
  import org.apache.flink.api.common.functions.Partitioner
  
  class MyPartitioner extends Partitioner[String]{
    override def partition(line: String, num: Int): Int = {
      println("åˆ†åŒºä¸ªæ•°ä¸º" +  num)
      if(line.contains("hello")){
        0
      }else{
        1
      }
    }
  }
  ~~~

- å®šä¹‰åˆ†åŒºclassç±»

  ~~~scala
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  
  object FlinkCustomerPartition {
    def main(args: Array[String]): Unit = {
      val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
      import  org.apache.flink.api.scala._
      //è·å–dataStream
      val sourceStream: DataStream[String] = environment.fromElements("hello laowang","spark flink","hello tony","hive hadoop")
        
      val rePartition: DataStream[String] = sourceStream.partitionCustom(new MyPartitioner,x => x +"")
      rePartition.map(x =>{
        println("æ•°æ®çš„keyä¸º" +  x + "çº¿ç¨‹ä¸º" + Thread.currentThread().getId)
        x
      })
      rePartition.print()
      environment.execute()
    }
  }
  ~~~





### ğŸ“–  5. DataSet è½¬æ¢ç®—å­

DataSetå®˜ç½‘è½¬æ¢ç®—å­æ“ä½œï¼š

https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/index.html#dataset-transformations



* Map
  * è¾“å…¥ä¸€ä¸ªå…ƒç´ ï¼Œç„¶åè¿”å›ä¸€ä¸ªå…ƒç´ ï¼Œä¸­é—´å¯ä»¥åšä¸€äº›æ¸…æ´—è½¬æ¢ç­‰æ“ä½œ

* FlatMap
  * è¾“å…¥ä¸€ä¸ªå…ƒç´ ï¼Œå¯ä»¥è¿”å›é›¶ä¸ªï¼Œä¸€ä¸ªæˆ–è€…å¤šä¸ªå…ƒç´ 

* MapPartition
  * ç±»ä¼¼mapï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ã€å¦‚æœåœ¨è¿›è¡Œmapå¤„ç†çš„æ—¶å€™éœ€è¦è·å–ç¬¬ä¸‰æ–¹èµ„æºé“¾æ¥ï¼Œå»ºè®®ä½¿ç”¨MapPartitionã€‘

* Filter
  * è¿‡æ»¤å‡½æ•°ï¼Œå¯¹ä¼ å…¥çš„æ•°æ®è¿›è¡Œåˆ¤æ–­ï¼Œç¬¦åˆæ¡ä»¶çš„æ•°æ®ä¼šè¢«ç•™ä¸‹

* Reduce
  * å¯¹æ•°æ®è¿›è¡Œèšåˆæ“ä½œï¼Œç»“åˆå½“å‰å…ƒç´ å’Œä¸Šä¸€æ¬¡reduceè¿”å›çš„å€¼è¿›è¡Œèšåˆæ“ä½œï¼Œç„¶åè¿”å›ä¸€ä¸ªæ–°çš„å€¼

* Aggregate
  * sumã€maxã€minç­‰
* Distinct
  * è¿”å›ä¸€ä¸ªæ•°æ®é›†ä¸­å»é‡ä¹‹åçš„å…ƒç´ ï¼Œdata.distinct()
* Join
  * å†…è¿æ¥
* OuterJoin
  * å¤–é“¾æ¥

* Cross
  * è·å–ä¸¤ä¸ªæ•°æ®é›†çš„ç¬›å¡å°”ç§¯
* Union
  * è¿”å›ä¸¤ä¸ªæ•°æ®é›†çš„æ€»å’Œï¼Œæ•°æ®ç±»å‹éœ€è¦ä¸€è‡´
* First-n
  * è·å–é›†åˆä¸­çš„å‰Nä¸ªå…ƒç´ 
* Sort Partition
  * åœ¨æœ¬åœ°å¯¹æ•°æ®é›†çš„æ‰€æœ‰åˆ†åŒºè¿›è¡Œæ’åºï¼Œé€šè¿‡sortPartition()çš„é“¾æ¥è°ƒç”¨æ¥å®Œæˆå¯¹å¤šä¸ªå­—æ®µçš„æ’åº



#### 5.1 mapPartition

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.scala.ExecutionEnvironment
import scala.collection.mutable.ArrayBuffer
import org.apache.flink.api.scala._

//todo: æµ‹è¯• mapPartition
object MapPartitionDataSet {
  def main(args: Array[String]): Unit = {
    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
    
    val arrayBuffer =ArrayBuffer[String]()
    arrayBuffer.+=("hello world1")
    arrayBuffer.+=("hello world2")
    arrayBuffer.+=("hello world3")
    arrayBuffer.+=("hello world4")

    val collectionDataSet: DataSet[String] = environment.fromCollection(arrayBuffer)

    val resultPartition: DataSet[String] = collectionDataSet.mapPartition(eachPartition => {
      eachPartition.map(eachLine => {
        val returnValue = eachLine + " result"
        returnValue
      })
    })
    resultPartition.print()

  }
}

```

#### 5.2 distinct

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.scala.ExecutionEnvironment
import scala.collection.mutable.ArrayBuffer
import org.apache.flink.api.scala._


//todo: æµ‹è¯• DistinctDataSet
object DistinctDataSet {
  def main(args: Array[String]): Unit = {

    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

    val arrayBuffer = new ArrayBuffer[String]()
    arrayBuffer.+=("hello world1")
    arrayBuffer.+=("hello world2")
    arrayBuffer.+=("hello world3")
    arrayBuffer.+=("hello world4")

    val collectionDataSet: DataSet[String] = environment.fromCollection(arrayBuffer)

    val dsDataSet: DataSet[String] = collectionDataSet.flatMap(x => x.split(" ")).distinct()
    dsDataSet.print()

  }
}
```

#### 5.3 join

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.scala.ExecutionEnvironment
import scala.collection.mutable.ArrayBuffer

//todo: æµ‹è¯• join
object JoinDataSet {
  def main(args: Array[String]): Unit = {

    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    val array1 = ArrayBuffer((1,"å¼ ä¸‰"),(2,"æå››"),(3,"ç‹äº”"))
    val array2 =ArrayBuffer((1,"18"),(2,"35"),(3,"42"))

    val firstDataStream: DataSet[(Int, String)] = environment.fromCollection(array1)
    val secondDataStream: DataSet[(Int, String)] = environment.fromCollection(array2)

    val joinResult: UnfinishedJoinOperation[(Int, String), (Int, String)] = firstDataStream.join(secondDataStream)

     //whereæŒ‡å®šå·¦è¾¹æµå…³è”çš„å­—æ®µ ï¼ŒequalToæŒ‡å®šä¸å³è¾¹æµç›¸åŒçš„å­—æ®µ
    val resultDataSet: DataSet[(Int, String, String)] = joinResult.where(0).equalTo(0).map(x => {
      (x._1._1, x._1._2, x._2._2)
    })


    resultDataSet.print()
  }
}

```

#### 5.4 leftOuterJoinã€rightOuterJoin

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.common.functions.JoinFunction
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.log4j.{Level, Logger}
import scala.collection.mutable.ArrayBuffer
import org.apache.flink.api.scala._

//todo: æµ‹è¯• leftOuterJoin å’Œ rightOuterJoin
object OutJoinDataSet {
  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {

    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment


    val array1 = ArrayBuffer((1,"å¼ ä¸‰"),(2,"æå››"),(3,"ç‹äº”"),(4,"å¼ é£"))
    val array2 =ArrayBuffer((1,"18"),(2,"35"),(3,"42"),(5,"50"))

    val firstDataStream: DataSet[(Int, String)] = environment.fromCollection(array1)
    val secondDataStream: DataSet[(Int, String)] = environment.fromCollection(array2)

    //å·¦å¤–è¿æ¥
    val leftOuterJoin: UnfinishedOuterJoinOperation[(Int, String), (Int, String)] = firstDataStream.leftOuterJoin(secondDataStream)

    //whereæŒ‡å®šå·¦è¾¹æµå…³è”çš„å­—æ®µ ï¼ŒequalToæŒ‡å®šä¸å³è¾¹æµç›¸åŒçš„å­—æ®µ
     val leftDataSet: JoinFunctionAssigner[(Int, String), (Int, String)] = leftOuterJoin.where(0).equalTo(0)


    //å¯¹å…³è”çš„æ•°æ®è¿›è¡Œå‡½æ•°æ“ä½œ
    val leftResult: DataSet[(Int, String,String)] = leftDataSet.apply(new JoinFunction[(Int, String), (Int, String), (Int,String, String)] {
      override def join(left: (Int, String), right: (Int, String)): (Int, String, String) = {
        val result = if (right == null) {
          Tuple3[Int, String, String](left._1, left._2, "null")
        } else {
          Tuple3[Int, String, String](left._1, left._2, right._2)
        }
        result
      }
    })

    leftResult.print()


    //å³å¤–è¿æ¥
    val rightOuterJoin: UnfinishedOuterJoinOperation[(Int, String), (Int, String)] = firstDataStream.rightOuterJoin(secondDataStream)

    //whereæŒ‡å®šå·¦è¾¹æµå…³è”çš„å­—æ®µ ï¼ŒequalToæŒ‡å®šä¸å³è¾¹æµç›¸åŒçš„å­—æ®µ
    val rightDataSet: JoinFunctionAssigner[(Int, String), (Int, String)] = rightOuterJoin.where(0).equalTo(0)


    //å¯¹å…³è”çš„æ•°æ®è¿›è¡Œå‡½æ•°æ“ä½œ
    val rightResult: DataSet[(Int, String,String)] = rightDataSet.apply(new JoinFunction[(Int, String), (Int, String), (Int,String, String)] {
      override def join(left: (Int, String), right: (Int, String)): (Int, String, String) = {
        val result = if (left == null) {
          Tuple3[Int, String, String](right._1, right._2, "null")
        } else {
          Tuple3[Int, String, String](right._1, right._2, left._2)
        }
        result
      }
    })

    rightResult.print()

  }
}

```



#### 5.5 cross

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.log4j.{Level, Logger}

import scala.collection.mutable.ArrayBuffer

//todo: æµ‹è¯• cross
object CrossJoinDataSet {

  Logger.getLogger("org").setLevel(Level.ERROR)
  def main(args: Array[String]): Unit = {
    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    val array1 = ArrayBuffer((1,"å¼ ä¸‰"),(2,"æå››"),(3,"ç‹äº”"),(4,"å¼ é£"))
    val array2 =ArrayBuffer((1,"18"),(2,"35"),(3,"42"),(5,"50"))

    val firstDataStream: DataSet[(Int, String)] = environment.fromCollection(array1)
    val secondDataStream: DataSet[(Int, String)] = environment.fromCollection(array2)

    //todo: crossç¬›å¡å°”ç§¯
    val crossDataSet: CrossDataSet[(Int, String), (Int, String)] = firstDataStream.cross(secondDataStream)

     crossDataSet.print()
  }
}

```

#### 5.6 first-n å’Œ sortPartition

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.common.operators.Order
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.log4j.{Level, Logger}
import org.apache.flink.api.scala._
import scala.collection.mutable.ArrayBuffer

//todo: æµ‹è¯• first å’Œ sortPartition
object TopNAndPartition {
  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {

    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
    
     //æ•°ç»„
    val array = ArrayBuffer((1,"å¼ 3",10),(2,"æ4",20),(3,"ç‹5",30),(3,"èµµ6",40))

    val collectionDataSet: DataSet[(Int, String,Int)] = environment.fromCollection(array)

     //è·å–å‰3ä¸ªå…ƒç´ 
     collectionDataSet.first(3).print()

    collectionDataSet
                    .groupBy(0) //æŒ‰ç…§ç¬¬ä¸€ä¸ªå­—æ®µè¿›è¡Œåˆ†ç»„
                    .sortGroup(2,Order.DESCENDING)  //æŒ‰ç…§ç¬¬ä¸‰ä¸ªå­—æ®µè¿›è¡Œæ’åº
                    .first(1)  //è·å–æ¯ç»„çš„å‰ä¸€ä¸ªå…ƒç´ 
                    .print()

    /**
      * ä¸åˆ†ç»„æ’åºï¼Œé’ˆå¯¹æ‰€æœ‰å…ƒç´ è¿›è¡Œæ’åºï¼Œç¬¬ä¸€ä¸ªå…ƒç´ é™åºï¼Œç¬¬ä¸‰ä¸ªå…ƒç´ å‡åº
      */
    collectionDataSet.sortPartition(0,Order.DESCENDING).sortPartition(2,Order.ASCENDING).print()


  }
}

```

#### 5.7 partitionåˆ†åŒºç®—å­

```scala
package com.kaikeba.batchOpreator

import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.log4j.{Level, Logger}

import scala.collection.mutable.ArrayBuffer

//todo: æµ‹è¯• partition
object PartitionDataSet {
   Logger.getLogger("org").setLevel(Level.ERROR)
  def main(args: Array[String]): Unit = {

    val environment: ExecutionEnvironment = ExecutionEnvironment.createLocalEnvironmentWithWebUI()
    import org.apache.flink.api.scala._

    //todo: å‡†å¤‡æ•°æ®é›†
    val array = ArrayBuffer((1,"hello"),
                            (2,"hello"),
                            (2,"hello"),
                            (3,"hello"),
                            (3,"hello"),
                            (3,"hello"),
                            (4,"hello"),
                            (4,"hello"),
                            (4,"hello"),
                            (4,"hello"),
                            (5,"hello"),
                            (5,"hello"),
                            (5,"hello"),
                            (5,"hello"),
                            (5,"hello"),
                            (6,"hello"),
                            (6,"hello"),
                            (6,"hello"),
                            (6,"hello"),
                            (6,"hello"),
                            (6,"hello"))
    environment.setParallelism(2)

    val sourceDataSet: DataSet[(Int, String)] = environment.fromCollection(array)

      // todo: partitionByHash:æŒ‰ç…§æŒ‡å®šçš„å­—æ®µhashPartitioneråˆ†åŒº
     sourceDataSet.partitionByHash(0).mapPartition(eachPartition => {
         eachPartition.foreach(t=>{
           println("å½“å‰çº¿ç¨‹IDä¸º" + Thread.currentThread().getId +"============="+t._1)
         })

        eachPartition
      }).print()

    sourceDataSet.partitionByRange()


     //partitionByRangeï¼šæŒ‰ç…§æŒ‡å®šçš„å­—æ®µè¿›è¡ŒèŒƒå›´åˆ†åŒº
//    sourceDataSet.partitionByRange(x => x._1).mapPartition(eachPartition =>{
//      eachPartition.foreach(t=>{
//        println("å½“å‰çº¿ç¨‹IDä¸º" + Thread.currentThread().getId +"============="+t._1)
//      })
//
//      eachPartition
//
//    }).print()
    
  }
}
```





### ğŸ“–  6.  Flinkçš„dataSet  connectorä»‹ç»

~~~
æŸ¥çœ‹å®˜ç½‘
https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/batch/connectors.html
~~~



#### 6.1 æ–‡ä»¶ç³»ç»Ÿconnector

* ä¸ºäº†ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ•°æ®ï¼ŒFlinkå†…ç½®äº†å¯¹ä»¥ä¸‹æ–‡ä»¶ç³»ç»Ÿçš„æ”¯æŒ:

| æ–‡ä»¶ç³»ç»Ÿ | Schema     | å¤‡æ³¨                       |
| -------- | ---------- | -------------------------- |
| HDFS     | hdfs://    | Hdfsæ–‡ä»¶ç³»ç»Ÿ               |
| S3       | s3://      | é€šè¿‡hadoopæ–‡ä»¶ç³»ç»Ÿå®ç°æ”¯æŒ |
| MapR     | maprfs://  | éœ€è¦ç”¨æˆ·æ·»åŠ jar            |
| Alluxio  | alluxio:// | é€šè¿‡hadoopæ–‡ä»¶ç³»ç»Ÿå®ç°     |

 

* æ³¨æ„
  * Flinkå…è®¸ç”¨æˆ·ä½¿ç”¨å®ç°org.apache.hadoop.fs.FileSystemæ¥å£çš„ä»»ä½•æ–‡ä»¶ç³»ç»Ÿã€‚ä¾‹å¦‚S3ã€ Google Cloud Storage Connector for Hadoopã€ Alluxioã€ XtreemFSã€ FTPç­‰å„ç§æ–‡ä»¶ç³»ç»Ÿ

 ~~~
Flinkä¸Apache Hadoop MapReduceæ¥å£å…¼å®¹ï¼Œå› æ­¤å…è®¸é‡ç”¨Hadoop MapReduceå®ç°çš„ä»£ç ï¼š

ä½¿ç”¨Hadoop Writable data type
ä½¿ç”¨ä»»ä½•Hadoop InputFormatä½œä¸ºDataSource(flinkå†…ç½®HadoopInputFormat)
ä½¿ç”¨ä»»ä½•Hadoop OutputFormatä½œä¸ºDataSink(flinkå†…ç½®HadoopOutputFormat)
ä½¿ç”¨Hadoop Mapperä½œä¸ºFlatMapFunction
ä½¿ç”¨Hadoop Reducerä½œä¸ºGroupReduceFunction
 ~~~





#### 6.2 Flinké›†æˆHbaseä¹‹æ•°æ®è¯»å–

Flinkä¹Ÿå¯ä»¥ç›´æ¥ä¸hbaseè¿›è¡Œé›†æˆï¼Œå°†hbaseä½œä¸ºFlinkçš„sourceå’Œsinkç­‰

* ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºhbaseè¡¨å¹¶æ’å…¥æ•°æ®

```shell
create 'hbasesource','f1'
put 'hbasesource','0001','f1:name','zhangsan'
put 'hbasesource','0001','f1:age','18'
```

 

* ç¬¬äºŒæ­¥ï¼šå¯¼å…¥æ•´åˆjaråŒ…

```xml

      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-client</artifactId>
          <version>3.1.4</version>
      </dependency>

      <dependency>
          <groupId>org.apache.flink</groupId>
          <artifactId>flink-hbase_2.11</artifactId>
          <!--ç›®å‰æä¾›çš„æœ€æ–°ç‰ˆæœ¬-->
          <version>1.10.2</version>
      </dependency>

```



* ç¬¬ä¸‰æ­¥ï¼šå¼€å‘flinké›†æˆhbaseè¯»å–hbaseæ•°æ®

```scala
package com.kaikeba.hbase

import org.apache.flink.addons.hbase.TableInputFormat
import org.apache.flink.api.java.tuple
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.configuration
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{Cell, HBaseConfiguration, HConstants, TableName}
import org.apache.log4j.{Level, Logger}
import org.apache.flink.api.scala._

/**
  * todo: flinkä»hbaseè¡¨ä¸­è¯»å–æ•°æ®
  */
object FlinkReadHBase {

  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {

        //todo: 1ã€è·å–æ‰¹å¤„ç†çš„ç¯å¢ƒ
       val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

       //todo: 2ã€é€šè¿‡InputFormatæ·»åŠ æ•°æ®æº
       val hbaseDataSet=env.createInput(new TableInputFormat[tuple.Tuple2[String, String]] {

         //åˆå§‹åŒ–é…ç½®æ–¹æ³•
         override def configure(parameters: configuration.Configuration): Unit = {
             val conf: Configuration = HBaseConfiguration.create()
             conf.set(HConstants.ZOOKEEPER_QUORUM, "node01,node02,node03")
             conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, "2181")
             val conn: Connection = ConnectionFactory.createConnection(conf)
             table = classOf[HTable].cast(conn.getTable(TableName.valueOf("hbasesource")))

           scan = new Scan() {
             addFamily(Bytes.toBytes("f1"))
           }
         }

        override def getTableName: String = {
          "hbasesource"
        }

        override def getScanner: Scan = {
          scan
        }

        //todo:3ã€è¯»å–hbaseè¡¨æ•°æ®
        override def mapResultToTuple(result: Result): tuple.Tuple2[String, String]  = {
          //è·å–rowkey
          val rowkey: String = Bytes.toString(result.getRow)
          val rawCells: Array[Cell] = result.rawCells()
          val sb = new StringBuffer()

          for (cell <- rawCells) {
            val value = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
            sb.append(value).append(",")
          }

          val valueString = sb.replace(sb.length() - 1, sb.length(), "").toString
          val tuple2 = new org.apache.flink.api.java.tuple.Tuple2[String, String]
          //ç»™å…ƒç´ çš„ä¸‹æ ‡èµ‹å€¼
          tuple2.setField(rowkey, 0)
          tuple2.setField(valueString, 1)
          tuple2

        }

    })

    //todo: 4ã€æ‰“å°è¾“å‡º
    hbaseDataSet.print()


  }
}

```



#### 6.3 Flinkè¯»å–æ•°æ®ï¼Œç„¶åå†™å…¥hbase

Flinkä¹Ÿå¯ä»¥é›†æˆHbaseå®ç°å°†æ•°æ®å†™å…¥åˆ°Hbaseé‡Œé¢å»

1. ç¬¬ä¸€ç§ï¼šå®ç°OutputFormatæ¥å£

2. ç¬¬äºŒç§ï¼šç»§æ‰¿RichSinkFunctioné‡å†™çˆ¶ç±»æ–¹æ³•

```scala
package com.kaikeba.hbase

import java.util

import org.apache.flink.api.common.io.OutputFormat
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.configuration.Configuration
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.flink.api.scala._

/**
  * todo: flinkå†™æ•°æ®åˆ°hbaseè¡¨ä¸­
  */
object FlinkWriteHBase {

  def main(args: Array[String]): Unit = {
    //todo: 1ã€è·å–æ‰¹å¤„ç†çš„ç¯å¢ƒ
    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

     //todo: 2ã€å‡†å¤‡æ•°æ®
    val sourceDataSet: DataSet[String] = environment.fromElements("0002,lisi,28","0003,wangwu,30")

    //todo: 3ã€ä½¿ç”¨OutputFormatæ¥å£ï¼Œå†™æ•°æ®åˆ°hbaseè¡¨ä¸­
    sourceDataSet.output(new HBaseOutputFormat)

    //todo: 4ã€å¯åŠ¨ä»»åŠ¡
    environment.execute()

  }
}

  //todo: å®šä¹‰OutputFormatæ¥å£
  class HBaseOutputFormat extends OutputFormat[String]{
    val zkServer = "node01,node02,node03"
    val port = "2181"
    var conn: Connection = null

    override def configure(parameters: Configuration): Unit = {}

    //todo: åˆå§‹åŒ–è·å–æ•°æ®åº“è¿æ¥
  override def open(taskNumber: Int, numTasks: Int): Unit = {
    val config: org.apache.hadoop.conf.Configuration = HBaseConfiguration.create
    config.set(HConstants.ZOOKEEPER_QUORUM, zkServer)
    config.set(HConstants.ZOOKEEPER_CLIENT_PORT, port)
    config.setInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 30000)
    config.setInt(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 30000)
    conn = ConnectionFactory.createConnection(config)
  }

  //todo: å†™æ•°æ®çš„æ–¹æ³•
  override def writeRecord(record: String): Unit ={
      val tableName: TableName = TableName.valueOf("hbasesource")
      val cf1 = "f1"
    //"0002,lisi,28"    "0003,wangwu,30"
      val array: Array[String] = record.split(",")
      val put: Put = new Put(Bytes.toBytes(array(0)))
      put.addColumn(Bytes.toBytes(cf1), Bytes.toBytes("name"), Bytes.toBytes(array(1)))
      put.addColumn(Bytes.toBytes(cf1), Bytes.toBytes("age"), Bytes.toBytes(array(2)))
      val puts = new util.ArrayList[Put]()

      puts.add(put)
      //è®¾ç½®ç¼“å­˜1mï¼Œå½“è¾¾åˆ°1mæ—¶æ•°æ®ä¼šè‡ªåŠ¨åˆ·åˆ°hbase
      val params: BufferedMutatorParams = new BufferedMutatorParams(tableName)
      //è®¾ç½®ç¼“å­˜çš„å¤§å°
      params.writeBufferSize(1024 * 1024)
      val mutator: BufferedMutator = conn.getBufferedMutator(params)
      mutator.mutate(puts)
      mutator.flush()
      puts.clear()
  }

  override def close(): Unit ={
    if(null != conn){
      conn.close()
    }
  }


}
```



### ğŸ“–  7.  Flinkä¹‹å¹¿æ’­å˜é‡

* æ¦‚å¿µ

~~~
	å¹¿æ’­å˜é‡å…è®¸ç¼–ç¨‹äººå‘˜åœ¨æ¯å°æœºå™¨ä¸Šä¿æŒä¸€ä¸ªåªè¯»çš„ç¼“å­˜å˜é‡ï¼Œè€Œä¸æ˜¯ä¼ é€å˜é‡çš„å‰¯æœ¬ç»™tasksï¼Œ
å¹¿æ’­å˜é‡åˆ›å»ºåï¼Œå®ƒå¯ä»¥è¿è¡Œåœ¨é›†ç¾¤ä¸­çš„ä»»ä½•functionä¸Šï¼Œè€Œä¸éœ€è¦å¤šæ¬¡ä¼ é€’ç»™é›†ç¾¤èŠ‚ç‚¹ã€‚å¦å¤–éœ€è¦è®°ä½ï¼Œä¸åº”è¯¥ä¿®æ”¹å¹¿æ’­å˜é‡ï¼Œè¿™æ ·æ‰èƒ½ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹è·å–åˆ°çš„å€¼éƒ½æ˜¯ä¸€è‡´çš„
	ä¸€å¥è¯è§£é‡Šï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯ä¸€ä¸ªå…¬å…±çš„å…±äº«å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä¸€ä¸ªdataset æ•°æ®é›†å¹¿æ’­å‡ºå»ï¼Œç„¶åä¸åŒçš„taskåœ¨èŠ‚ç‚¹ä¸Šéƒ½èƒ½å¤Ÿè·å–åˆ°ï¼Œè¿™ä¸ªæ•°æ®åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåªä¼šå­˜åœ¨ä¸€ä»½ã€‚
	å¦‚æœä¸ä½¿ç”¨broadcastï¼Œåˆ™åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸­çš„æ¯ä¸ªtaskä¸­éƒ½éœ€è¦æ‹·è´ä¸€ä»½datasetæ•°æ®é›†ï¼Œæ¯”è¾ƒæµªè´¹å†…å­˜(ä¹Ÿå°±æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ä¸­å¯èƒ½ä¼šå­˜åœ¨å¤šä»½datasetæ•°æ®)ã€‚
~~~

* ç”¨æ³•

~~~scala
ï¼ˆ1ï¼‰ï¼šåˆå§‹åŒ–æ•°æ®
DataSet<Integer> toBroadcast = env.fromElements(1, 2, 3)

ï¼ˆ2ï¼‰ï¼šå¹¿æ’­æ•°æ®
.withBroadcastSet(toBroadcast, "broadcastSetName");

ï¼ˆ3ï¼‰ï¼šè·å–æ•°æ®
Collection<Integer> broadcastSet = getRuntimeContext().getBroadcastVariable("broadcastSetName");

æ³¨æ„ï¼š
aï¼šå¹¿æ’­å‡ºå»çš„å˜é‡å­˜åœ¨äºæ¯ä¸ªèŠ‚ç‚¹çš„å†…å­˜ä¸­ï¼Œæ‰€ä»¥è¿™ä¸ªæ•°æ®é›†ä¸èƒ½å¤ªå¤§ã€‚å› ä¸ºå¹¿æ’­å‡ºå»çš„æ•°æ®ï¼Œä¼šå¸¸é©»å†…å­˜ï¼Œé™¤éç¨‹åºæ‰§è¡Œç»“æŸ
bï¼šå¹¿æ’­å˜é‡åœ¨åˆå§‹åŒ–å¹¿æ’­å‡ºå»ä»¥åä¸æ”¯æŒä¿®æ”¹ï¼Œè¿™æ ·æ‰èƒ½ä¿è¯æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®éƒ½æ˜¯ä¸€è‡´çš„ã€‚
~~~

* æ¡ˆä¾‹

```scala
package com.kaikeba.batchOpreator.broadcast

import org.apache.flink.api.common.functions.RichMapFunction
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.configuration.Configuration
import org.apache.log4j.{Level, Logger}
import org.apache.flink.api.scala._
import scala.collection.mutable.ArrayBuffer

/**
  * todo: flinkå¹¿æ’­å˜é‡ä½¿ç”¨æ¡ˆä¾‹
  */
object FlinkBroadCast {
  Logger.getLogger("org").setLevel(Level.ERROR)
  def main(args: Array[String]): Unit = {
    //todo: æ„å»ºæµå¤„ç†ç¯å¢ƒ
    val environment: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment

    //todo: å‡†å¤‡æ•°æ®é›†
    val userInfo =ArrayBuffer(("zs", 10),("ls", 20),("ww", 30))

     //todo: åŠ è½½æ•°æ®é›†æ„å»ºDataSet--éœ€è¦å¹¿æ’­çš„æ•°æ®
     val userDataSet: DataSet[(String, Int)] = environment.fromCollection(userInfo)

      //åŸå§‹æ•°æ®
      val data = environment.fromElements("zs","ls","ww","zl")


     //todo: åœ¨è¿™é‡Œéœ€è¦ä½¿ç”¨åˆ°RichMapFunctionè·å–å¹¿æ’­å˜é‡   //ä½¿ç”¨å¯Œå‡½æ•°ç±»å®ç°ä¸€äº›å¤æ‚çš„åŠŸèƒ½
    val result = data.map(new RichMapFunction[String,String] {
        //å®šä¹‰ä¸€ä¸ªmapé›†åˆï¼Œå­˜å‚¨å¹¿æ’­å˜é‡ä¸­çš„å†…å®¹
        var allMap  = Map[String,Int]()

          //åˆå§‹åŒ–æ–¹æ³•  å¯ä»¥åœ¨openæ–¹æ³•ä¸­è·å–å¹¿æ’­å˜é‡æ•°æ®
        override def open(parameters: Configuration): Unit ={
            //è·å–å¹¿æ’­å˜é‡(broadcastMapName)çš„å€¼
          val  listData= getRuntimeContext.getBroadcastVariable[(String,Int)]("broadcastMapName")

          val it = listData.iterator()

          while (it.hasNext){
            val tuple = it.next()
            allMap +=(tuple._1 -> tuple._2)
          }
        }
         //ä½¿ç”¨å¹¿æ’­å˜é‡æ“ä½œæ•°æ®
        override def map(name: String): String = {
          val age = allMap.getOrElse(name,20)
          name+","+age
        }
      }).withBroadcastSet(userDataSet,"broadcastMapName")


    result.print()

  }
}

```





### ğŸ“–  8.  Flinkä¹‹Counterï¼ˆè®¡æ•°å™¨/ç´¯åŠ å™¨ï¼‰

* æ¦‚å¿µ

```
	Accumulatorå³ç´¯åŠ å™¨ï¼Œä¸Mapreduce counterçš„åº”ç”¨åœºæ™¯å·®ä¸å¤šï¼Œéƒ½èƒ½å¾ˆå¥½åœ°è§‚å¯Ÿtaskåœ¨è¿è¡ŒæœŸé—´çš„æ•°æ®å˜åŒ–ï¼Œå¯ä»¥åœ¨Flink jobä»»åŠ¡ä¸­çš„ç®—å­å‡½æ•°ä¸­æ“ä½œç´¯åŠ å™¨ï¼Œä½†æ˜¯åªèƒ½åœ¨ä»»åŠ¡æ‰§è¡Œç»“æŸä¹‹åæ‰èƒ½è·å¾—ç´¯åŠ å™¨çš„æœ€ç»ˆç»“æœã€‚
Counteræ˜¯ä¸€ä¸ªå…·ä½“çš„ç´¯åŠ å™¨(Accumulator)å®ç°IntCounter, LongCounter å’Œ DoubleCounter
```

* ç”¨æ³•

~~~scala
(1)ï¼šåˆ›å»ºç´¯åŠ å™¨
val counter=new IntCounter()

(2)ï¼šæ³¨å†Œç´¯åŠ å™¨
getRuntimeContext.addAccumulator("num-lines",counter)

(3)ï¼šä½¿ç”¨ç´¯åŠ å™¨
counter.add(1)

(4)ï¼šè·å–ç´¯åŠ å™¨çš„ç»“æœ
myJobExecutionResult.getAccumulatorResult("num-lines")

~~~

* æ¡ˆä¾‹
  * éœ€æ±‚
    * é€šè¿‡è®¡æ•°å™¨æ¥å®ç°ç»Ÿè®¡æ–‡ä»¶å½“ä¸­Exceptionå…³é”®å­—å‡ºç°çš„æ¬¡æ•°

```java
package com.kaikeba.batchOpreator.accumulator

import org.apache.flink.api.common.JobExecutionResult
import org.apache.flink.api.common.accumulators.LongCounter
import org.apache.flink.api.common.functions.RichMapFunction
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.configuration.Configuration
import org.apache.log4j.{Level, Logger}
import org.apache.flink.api.scala._

//todo:  é€šè¿‡è®¡æ•°å™¨æ¥å®ç°ç»Ÿè®¡æ–‡ä»¶å½“ä¸­Exceptionå…³é”®å­—å‡ºç°çš„æ¬¡æ•°
object FlinkCounterAndAccumulator {

  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {
    //todo: 1ã€è·å–æ‰¹å¤„ç†çš„ç¯å¢ƒ
    val env=ExecutionEnvironment.getExecutionEnvironment

    //todo: 2ã€åŠ è½½æ•°æ®ï¼Œç»Ÿè®¡tomcatæ—¥å¿—å½“ä¸­exceptionå…³é”®å­—å‡ºç°äº†å¤šå°‘æ¬¡
    val sourceDataSet: DataSet[String] = env.readTextFile("D:\\catalina.out")

    //todo: 3ã€æ•°æ®å¤„ç†
      sourceDataSet.map(new RichMapFunction[String,String] {
         //åˆ›å»ºç´¯åŠ å™¨
        val counter=new LongCounter()

        override def open(parameters: Configuration): Unit = {
          //æ³¨å†Œç´¯åŠ å™¨
          getRuntimeContext.addAccumulator("my-accumulator",counter)
        }

        //å®ç°ä¸šåŠ¡é€»è¾‘
      override def map(value: String): String = {
        if(value.toLowerCase().contains("exception")){
           //æ»¡è¶³æ¡ä»¶ç´¯åŠ å™¨åŠ 1
          counter.add(1)
        }
        value
      }
    }).writeAsText("D:\\test123")

    val job: JobExecutionResult = env.execute()

    //todo: 4ã€è·å–ç´¯åŠ å™¨ï¼Œå¹¶æ‰“å°ç´¯åŠ å™¨çš„å€¼
    val count=job.getAccumulatorResult[Long]("my-accumulator")

     //todo: 5ã€æ‰“å°
    println(count)


  }

}

```



### ğŸ“–  9.  åˆ†å¸ƒå¼ç¼“å­˜

* æ¦‚å¿µ

~~~
	Flinkæä¾›äº†ä¸€ä¸ªç±»ä¼¼äºhadoopåˆ†å¸ƒå¼ç¼“å­˜ï¼Œå¯ä»¥ä½¿ç”¨æˆ·åœ¨å¹¶è¡Œå‡½æ•°ä¸­å¾ˆæ–¹ä¾¿çš„è¯»å–æœ¬åœ°æ–‡ä»¶ã€‚
å‰é¢è®²åˆ°çš„å¹¿æ’­å˜é‡æ˜¯å°†ä¸€äº›å…±äº«çš„æ•°æ®æ”¾åœ¨TaskManagerå†…å­˜ä¸­ï¼Œè€ŒDistribute cacheæ˜¯ä»å¤–éƒ¨åŠ è½½ä¸€ä¸ªæ–‡ä»¶/ç›®å½•(ä¾‹å¦‚hdfs)ï¼Œç„¶ååˆ†åˆ«å¤åˆ¶åˆ°æ¯ä¸€ä¸ªTaskManagerçš„æœ¬åœ°ç£ç›˜ä¸­ã€‚
~~~

* ç”¨æ³•

~~~scala
(1)ï¼šä½¿ç”¨Flinkè¿è¡Œç¯å¢ƒè°ƒç”¨registerCachedFileæ³¨å†Œä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜
env.registerCachedFile("hdfs:///path/to/your/file", "hdfsFile")  

(2): è·å–åˆ†å¸ƒå¼ç¼“å­˜
File myFile = getRuntimeContext().getDistributedCache().getFile("hdfsFile");
~~~

* æ¡ˆä¾‹

```scala
package com.kaikeba.batchOpreator.distributedCache

import org.apache.commons.io.FileUtils
import org.apache.flink.api.common.functions.RichMapFunction
import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.configuration.Configuration
import org.apache.log4j.{Level, Logger}
import org.apache.flink.api.scala._

/**
  * todoï¼š flinkçš„åˆ†å¸ƒå¼ç¼“å­˜ä½¿ç”¨
  */
object FlinkDistributedCache {
  Logger.getLogger("org").setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = {

    //todo: æ„å»ºExecutionEnvironment
      val env = ExecutionEnvironment.getExecutionEnvironment

      //å‡†å¤‡æ•°æ®é›†                        todo: å­¦ç”ŸID, å­¦ç§‘, åˆ†æ•°    å°†ï¼ˆå­¦ç”ŸID, å­¦ç§‘, åˆ†æ•°ï¼‰è½¬æ¢ä¸ºï¼ˆå­¦ç”Ÿå§“åï¼Œå­¦ç§‘ï¼Œåˆ†æ•°ï¼‰
      val scoreDataSet  = env.fromElements((1, "è¯­æ–‡", 50),(2, "æ•°å­¦", 60), (3, "è‹±æ–‡", 80))

      //todo:1ã€æ³¨å†Œåˆ†å¸ƒå¼ç¼“å­˜æ–‡ä»¶
      env.registerCachedFile("D:\\distribute_cache_student.txt","student")

     //todoï¼š2ã€å¯¹æˆç»©æ•°æ®é›†è¿›è¡Œmapè½¬æ¢ï¼Œå°†ï¼ˆå­¦ç”ŸID, å­¦ç§‘, åˆ†æ•°ï¼‰è½¬æ¢ä¸ºï¼ˆå­¦ç”Ÿå§“åï¼Œå­¦ç§‘ï¼Œåˆ†æ•°ï¼‰
    val result: DataSet[(String, String, Int)] = scoreDataSet.map(
                                  new RichMapFunction[(Int, String, Int), (String, String, Int)] {

            //todo: å­˜å‚¨åˆ†å¸ƒå¼ç¼“å­˜æ–‡ä»¶
            var list: List[(Int, String)] = _

            //todo: åˆå§‹åŒ–æ–¹æ³•
            override def open(parameters: Configuration): Unit = {

              //todo: è·å–åˆ†å¸ƒå¼ç¼“å­˜çš„æ–‡ä»¶
              val file = getRuntimeContext.getDistributedCache.getFile("student")

              //todo: è·å–æ–‡ä»¶çš„å†…å®¹
               import scala.collection.JavaConverters._
               val listData: List[String] = FileUtils.readLines(file).asScala.toList
              //todo: å°†æ–‡æœ¬è½¬æ¢ä¸ºå…ƒç»„ï¼ˆå­¦ç”ŸIDï¼Œå­¦ç”Ÿå§“å)
              list = listData.map {
                line =>{
                  val array = line.split(",")
                  (array(0).toInt, array(1))
                }
              }

            }

            //todo: åœ¨mapæ–¹æ³•ä¸­ä½¿ç”¨åˆ†å¸ƒå¼ç¼“å­˜æ•°æ®è¿›è¡Œè½¬æ¢
                                     //idï¼Œå­¦ç§‘ï¼Œæˆç»©   ----->å§“åï¼Œå­¦ç§‘ï¼Œæˆç»©
            override def map(value: (Int, String, Int)): (String, String, Int) = {
              //è·å–å­¦ç”Ÿid
              val studentId: Int = value._1
              val studentName: String = list.filter(x => studentId == x._1)(0)._2

              //å°è£…ç»“æœè¿”å›
              // å°†æˆç»©æ•°æ®(å­¦ç”ŸIDï¼Œå­¦ç§‘ï¼Œæˆç»©) -> (å­¦ç”Ÿå§“åï¼Œå­¦ç§‘ï¼Œæˆç»©)
              (studentName, value._2, value._3)

            }
    })

     result.print()


    }
}

```



### ğŸ“–  10. Flinkçš„taskä¹‹é—´æ•°æ®ä¼ è¾“ç­–ç•¥ä»¥åŠOperator Chain

#### 10.1 æ•°æ®ä¼ è¾“ç­–ç•¥

* forward strategy

  * è½¬å‘ç­–ç•¥

  ~~~
  ï¼ˆ1ï¼‰ ä¸€ä¸ª task çš„è¾“å‡ºåªå‘é€ç»™ä¸€ä¸ª task ä½œä¸ºè¾“å…¥
  ï¼ˆ2ï¼‰ å¦‚æœä¸¤ä¸ª task éƒ½åœ¨ä¸€ä¸ª JVM ä¸­çš„è¯ï¼Œé‚£ä¹ˆå°±å¯ä»¥é¿å…ç½‘ç»œå¼€é”€
  ~~~

![1587367817075](/images/1587367817075.png)



* key-based strategy 

  * åŸºäºé”®çš„ç­–ç•¥

  ~~~
  ï¼ˆ1ï¼‰æ•°æ®éœ€è¦æŒ‰ç…§æŸä¸ªå±æ€§(æˆ‘ä»¬ç§°ä¸º key)è¿›è¡Œåˆ†ç»„(æˆ–è€…è¯´åˆ†åŒº)
  ï¼ˆ2ï¼‰ç›¸åŒkeyçš„æ•°æ®éœ€è¦ä¼ è¾“ç»™åŒä¸€ä¸ªtaskï¼Œåœ¨ä¸€ä¸ªtaskä¸­è¿›è¡Œå¤„ç†
  ~~~

![1587368188020](/images/1587368188020.png)

 

* broadcast strategy

  * å¹¿æ’­ç­–ç•¥

  ~~~
  ï¼ˆ1ï¼‰åœ¨è¯¥æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ•°æ®é›†ä¸åŠ¨ï¼Œå¦ä¸€ä¸ªæ•°æ®é›†ä¼šcopyåˆ°æœ‰ç¬¬ä¸€ä¸ªæ•°æ®é›†éƒ¨åˆ†æ•°æ®çš„æ‰€æœ‰æœºå™¨ä¸Šã€‚
  	å¦‚æœä½¿ç”¨å°æ•°æ®é›†ä¸å¤§æ•°æ®é›†è¿›è¡Œjoinï¼Œå¯ä»¥é€‰æ‹©broadcast-forwardç­–ç•¥ï¼Œå°†å°æ•°æ®é›†å¹¿æ’­ï¼Œé¿å…ä»£ä»·é«˜çš„é‡åˆ†åŒºã€‚
  ~~~

![1587368455285](/images/1587368455285.png)



* random strategy

  * éšæœºç­–ç•¥

  ~~~
  ï¼ˆ1ï¼‰æ•°æ®éšæœºçš„ä»ä¸€ä¸ªtaskä¸­ä¼ è¾“ç»™ä¸‹ä¸€ä¸ªoperatoræ‰€æœ‰çš„subtask
  ~~~

![1587368734607](/images/1587368734607.png)

PS: 

 è½¬å‘ä¸éšæœºç­–ç•¥æ˜¯åŸºäºkey-basedç­–ç•¥çš„ï¼›è½¬å‘ç­–ç•¥å’Œéšæœºç­–ç•¥ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯åŸºäºé”®çš„ç­–ç•¥çš„å˜ä½“ï¼Œå…¶ä¸­å‰è€…ä¿å­˜ä¸Šæ¸¸å…ƒç»„çš„é”®ï¼Œè€Œåè€…æ‰§è¡Œé”®çš„éšæœºé‡æ–°åˆ†é…ã€‚



#### 10.2 Operator Chain

* æ¦‚å¿µ 

  > â€‹	operator chainæ˜¯æŒ‡å°†æ»¡è¶³ä¸€å®šæ¡ä»¶çš„`operator`é“¾åœ¨ä¸€èµ·ï¼Œæ”¾åœ¨åŒä¸€ä¸ªtaské‡Œé¢æ‰§è¡Œï¼Œæ˜¯Flinkä»»åŠ¡ä¼˜åŒ–çš„ä¸€ç§æ–¹å¼ï¼Œåœ¨åŒä¸€ä¸ªtaské‡Œé¢çš„operatorçš„æ•°æ®ä¼ è¾“å˜æˆå‡½æ•°è°ƒç”¨å…³ç³»ï¼Œ`å®ƒèƒ½å‡å°‘çº¿ç¨‹ä¹‹é—´çš„åˆ‡æ¢ï¼Œå‡å°‘æ¶ˆæ¯çš„åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼Œå‡å°‘æ•°æ®åœ¨ç¼“å†²åŒºçš„äº¤æ¢ï¼Œå‡å°‘äº†å»¶è¿Ÿçš„åŒæ—¶æé«˜æ•´ä½“çš„ååé‡ã€‚`
  >
  > â€‹	å¸¸è§çš„chainï¼Œä¾‹å¦‚ï¼šsource->map->filterï¼Œè¿™æ ·çš„ä»»åŠ¡é“¾å¯ä»¥chainåœ¨ä¸€èµ·ï¼Œé‚£ä¹ˆå…¶å†…éƒ¨æ˜¯å¦‚ä½•å†³å®šæ˜¯å¦èƒ½å¤Ÿchainåœ¨ä¸€èµ·çš„å‘¢ï¼Ÿ
  
* Operator Chainçš„æ¡ä»¶

  ~~~
  ï¼ˆ1ï¼‰ æ•°æ®ä¼ è¾“ç­–ç•¥æ˜¯ forward strategy
  ï¼ˆ2ï¼‰ åœ¨åŒä¸€ä¸ªTaskManagerä¸­è¿è¡Œ
   (3) ä¸Šä¸‹æ¸¸taskçš„å¹¶è¡Œåº¦ç›¸åŒ
  ~~~


* åœ¨æˆ‘ä»¬çš„å•è¯æŠ€æœ¯ç»Ÿè®¡ç¨‹åºå½“ä¸­ï¼Œè®¾ç½®å¯¹åº”çš„å¹¶è¡Œåº¦ï¼Œä¾¿ä¼šå‘ç”Ÿoperator chainè¿™ä¸ªåŠ¨ä½œäº†

![1587380754504](/images/1587380754504.png)

![1587380798047](/images/1587380798047.png)



![1587380857265](/images/1587380857265.png)

![image-20200924175356031](/image-images/20200924175356031.png)



### ğŸ“–  11. Flinkå››å±‚æ¨¡å‹

* Flink ä¸­çš„æ‰§è¡Œå›¾å¯ä»¥åˆ†æˆå››å±‚ï¼šStreamGraph -> JobGraph -> ExecutionGraph -> ç‰©ç†æ‰§è¡Œå›¾ã€‚
  *  ==Stream Graph==
    * æ˜¯æ ¹æ®ç”¨æˆ·é€šè¿‡ Stream API ç¼–å†™çš„ä»£ç ç”Ÿæˆçš„æœ€åˆçš„å›¾ã€‚ç”¨æ¥è¡¨ç¤ºç¨‹åºçš„æ‹“æ‰‘ç»“æ„ã€‚
  * ==Job Graph==
    * StreamGraphç»è¿‡ä¼˜åŒ–åç”Ÿæˆäº† JobGraphï¼Œæäº¤ç»™ JobManager çš„æ•°æ®ç»“æ„ã€‚ä¸»è¦çš„ä¼˜åŒ–ä¸ºå°†å¤šä¸ªç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ chain åœ¨ä¸€èµ·ä½œä¸ºä¸€ä¸ªèŠ‚ç‚¹ã€‚
  *  ==Execution Graph==
    * JobManager æ ¹æ® JobGraph ç”ŸæˆExecutionGraphã€‚ ExecutionGraphæ˜¯JobGraphçš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬ï¼Œæ˜¯è°ƒåº¦å±‚æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„ã€‚
  *  ==Physical Execution Graph==
    * JobManager æ ¹æ® ExecutionGraph å¯¹ Job è¿›è¡Œè°ƒåº¦åï¼Œåœ¨å„ä¸ª TaskManager ä¸Šéƒ¨ç½² Task åå½¢æˆçš„â€œå›¾â€ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„æ•°æ®ç»“æ„ã€‚





![20200328221402552](/images/20200328221402552.png)



![img](/images/5501600.png)



#### 11.1 StreamGraph

* æ ¹æ®ç”¨æˆ·é€šè¿‡ Stream API ç¼–å†™çš„ä»£ç ç”Ÿæˆçš„æœ€åˆçš„å›¾ã€‚
  - StreamNodeï¼šç”¨æ¥ä»£è¡¨ operator çš„ç±»ï¼Œå¹¶å…·æœ‰æ‰€æœ‰ç›¸å…³çš„å±æ€§ï¼Œå¦‚å¹¶å‘åº¦ã€å…¥è¾¹å’Œå‡ºè¾¹ç­‰ã€‚
  - StreamEdgeï¼šè¡¨ç¤ºè¿æ¥ä¸¤ä¸ªStreamNodeçš„è¾¹ã€‚



#### 11.2 JobGraph

* StreamGraphç»è¿‡ä¼˜åŒ–åç”Ÿæˆäº† JobGraphï¼Œæäº¤ç»™ JobManager çš„æ•°æ®ç»“æ„ã€‚
  - JobVertexï¼šç»è¿‡ä¼˜åŒ–åç¬¦åˆæ¡ä»¶çš„å¤šä¸ªStreamNodeå¯èƒ½ä¼šchainåœ¨ä¸€èµ·ç”Ÿæˆä¸€ä¸ªJobVertexï¼Œå³ä¸€ä¸ªJobVertexåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªoperatorï¼ŒJobVertexçš„è¾“å…¥æ˜¯JobEdgeï¼Œè¾“å‡ºæ˜¯IntermediateDataSetã€‚
  - IntermediateDataSetï¼šè¡¨ç¤ºJobVertexçš„è¾“å‡ºï¼Œå³ç»è¿‡operatorå¤„ç†äº§ç”Ÿçš„æ•°æ®é›†ã€‚produceræ˜¯JobVertexï¼Œconsumeræ˜¯JobEdgeã€‚
  - JobEdgeï¼šä»£è¡¨äº†job graphä¸­çš„ä¸€æ¡æ•°æ®ä¼ è¾“é€šé“ã€‚source æ˜¯ IntermediateDataSetï¼Œtarget æ˜¯ JobVertexã€‚å³æ•°æ®é€šè¿‡JobEdgeç”±IntermediateDataSetä¼ é€’ç»™ç›®æ ‡JobVertexã€‚



#### 11.3 ExecutionGraph

* JobManager æ ¹æ® JobGraph ç”ŸæˆExecutionGraphã€‚ExecutionGraphæ˜¯JobGraphçš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬ï¼Œæ˜¯è°ƒåº¦å±‚æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„ã€‚
  - ExecutionJobVertexï¼šå’ŒJobGraphä¸­çš„JobVertexä¸€ä¸€å¯¹åº”ã€‚æ¯ä¸€ä¸ªExecutionJobVertexéƒ½æœ‰å’Œå¹¶å‘åº¦ä¸€æ ·å¤šçš„ ExecutionVertexã€‚
  - ExecutionVertexï¼šè¡¨ç¤ºExecutionJobVertexçš„å…¶ä¸­ä¸€ä¸ªå¹¶å‘å­ä»»åŠ¡ï¼Œè¾“å…¥æ˜¯ExecutionEdgeï¼Œè¾“å‡ºæ˜¯IntermediateResultPartitionã€‚
  - IntermediateResultï¼šå’ŒJobGraphä¸­çš„IntermediateDataSetä¸€ä¸€å¯¹åº”ã€‚ä¸€ä¸ªIntermediateResultåŒ…å«å¤šä¸ªIntermediateResultPartitionï¼Œå…¶ä¸ªæ•°ç­‰äºè¯¥operatorçš„å¹¶å‘åº¦ã€‚
  - IntermediateResultPartitionï¼šè¡¨ç¤ºExecutionVertexçš„ä¸€ä¸ªè¾“å‡ºåˆ†åŒºï¼Œproduceræ˜¯ExecutionVertexï¼Œconsumeræ˜¯è‹¥å¹²ä¸ªExecutionEdgeã€‚
  - ExecutionEdgeï¼šè¡¨ç¤ºExecutionVertexçš„è¾“å…¥ï¼Œsourceæ˜¯IntermediateResultPartitionï¼Œtargetæ˜¯ ExecutionVertexã€‚sourceå’Œtargetéƒ½åªèƒ½æ˜¯ä¸€ä¸ªã€‚
  - Executionï¼šæ˜¯æ‰§è¡Œä¸€ä¸ª ExecutionVertex çš„ä¸€æ¬¡å°è¯•ã€‚å½“å‘ç”Ÿæ•…éšœæˆ–è€…æ•°æ®éœ€è¦é‡ç®—çš„æƒ…å†µä¸‹ ExecutionVertex å¯èƒ½ä¼šæœ‰å¤šä¸ª ExecutionAttemptIDã€‚ä¸€ä¸ª Execution é€šè¿‡ ExecutionAttemptID æ¥å”¯ä¸€æ ‡è¯†ã€‚JMå’ŒTMä¹‹é—´å…³äº task çš„éƒ¨ç½²å’Œ task status çš„æ›´æ–°éƒ½æ˜¯é€šè¿‡ ExecutionAttemptID æ¥ç¡®å®šæ¶ˆæ¯æ¥å—è€…ã€‚



#### 11.4 ç‰©ç†æ‰§è¡Œå›¾

* JobManager æ ¹æ® ExecutionGraph å¯¹ Job è¿›è¡Œè°ƒåº¦åï¼Œåœ¨å„ä¸ªTaskManager ä¸Šéƒ¨ç½² Task åå½¢æˆçš„â€œå›¾â€ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„æ•°æ®ç»“æ„ã€‚
  - Taskï¼šExecutionè¢«è°ƒåº¦ååœ¨åˆ†é…çš„ TaskManager ä¸­å¯åŠ¨å¯¹åº”çš„ Taskã€‚Task åŒ…è£¹äº†å…·æœ‰ç”¨æˆ·æ‰§è¡Œé€»è¾‘çš„ operatorã€‚
  - ResultPartitionï¼šä»£è¡¨ç”±ä¸€ä¸ªTaskçš„ç”Ÿæˆçš„æ•°æ®ï¼Œå’ŒExecutionGraphä¸­çš„IntermediateResultPartitionä¸€ä¸€å¯¹åº”ã€‚
  - ResultSubpartitionï¼šæ˜¯ResultPartitionçš„ä¸€ä¸ªå­åˆ†åŒºã€‚æ¯ä¸ªResultPartitionåŒ…å«å¤šä¸ªResultSubpartitionï¼Œå…¶æ•°ç›®è¦ç”±ä¸‹æ¸¸æ¶ˆè´¹ Task æ•°å’Œ DistributionPattern æ¥å†³å®šã€‚
  - InputGateï¼šä»£è¡¨Taskçš„è¾“å…¥å°è£…ï¼Œå’ŒJobGraphä¸­JobEdgeä¸€ä¸€å¯¹åº”ã€‚æ¯ä¸ªInputGateæ¶ˆè´¹äº†ä¸€ä¸ªæˆ–å¤šä¸ªçš„ResultPartitionã€‚
  - InputChannelï¼šæ¯ä¸ªInputGateä¼šåŒ…å«ä¸€ä¸ªä»¥ä¸Šçš„InputChannelï¼Œå’ŒExecutionGraphä¸­çš„ExecutionEdgeä¸€ä¸€å¯¹åº”ï¼Œä¹Ÿå’ŒResultSubpartitionä¸€å¯¹ä¸€åœ°ç›¸è¿ï¼Œå³ä¸€ä¸ªInputChannelæ¥æ”¶ä¸€ä¸ªResultSubpartitionçš„è¾“å‡ºã€‚



